{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework4.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "JCNjIhyJIBdt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Part 1"
      ]
    },
    {
      "metadata": {
        "id": "aRRe7oRlCRJN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I believe deep neural networks are a breakthrough that will be used years in the future, however I don't think they can solve some of the more complex problems that can't be reduced to statistics. It will probably be used for years to come because it has already been shown that it can quickly recognize different things and different patterns. This can probably get it very far. For example, self driving cars and game playing AIs are just the start of the type of problems it can solve.\n",
        "\n",
        "However, I believe the model has its limits. It can only learn from previous examples which tell it how to learn, in supervised learning, but it can't extrapolate much further than that. Even with unsupervised learning it can only go as far as the data it's seen and can only accomplish very specific goals. My point is, supervised or unsupervised, networks usually have narrowly defined goals (winning a game, not hitting a car in front of it, staying between lines while driving, etc). However, some of the greatest work to be done is on frontiers where goals are unclear, so I'm not sure if deep neural nets can be abstracted enough to infer it's own goals and whether or not those goals are worthy. Sure, a collection of smaller goals can solve a larger more abstract problem, but human still need to choose the goals and even the ultimate goal the subgoals are being guided towards."
      ]
    },
    {
      "metadata": {
        "id": "8OhntKo-8QI4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  Part 2, this is heavily based on the class example (most is copied and pasted).\n",
        "  \n",
        "  $\\begin{aligned}\n",
        "    &\\begin{bmatrix}\n",
        "    w_{i_1,h_1} & w_{i_1,h_2} \\\\\n",
        "    w_{i_2,h_1} & w_{i_2,h_2}\n",
        "    \\end{bmatrix}\n",
        "    \\leftarrow\n",
        "    \\begin{bmatrix}\n",
        "    0.11 & 0.12 \\\\\n",
        "    0.21 & 0.08\n",
        "    \\end{bmatrix} \\\\\n",
        "    &\\begin{bmatrix}\n",
        "    w_{h_1, o_1} \\\\ \n",
        "    w_{h_2, o_1} \n",
        "    \\end{bmatrix}\n",
        "    \\leftarrow\n",
        "    \\begin{bmatrix}\n",
        "    0.14 \\\\\n",
        "    0.15\n",
        "    \\end{bmatrix}\n",
        "    \\end{aligned}$\n",
        "    \n",
        "2. Compute the output for one sample (XOR: `[1, 1]` &rarr; `0`).\n",
        "\n",
        "    $\\begin{aligned}\n",
        "    o_j &= \n",
        "    \\begin{bmatrix}\n",
        "    1 & 1 \\\\ \n",
        "    \\end{bmatrix}\n",
        "    \\cdot\n",
        "    \\begin{bmatrix}\n",
        "    0.11 & 0.12 \\\\\n",
        "    0.21 & 0.08\n",
        "    \\end{bmatrix}\n",
        "    \\cdot\n",
        "    \\begin{bmatrix}\n",
        "    0.14 \\\\\n",
        "    0.15\n",
        "    \\end{bmatrix}\n",
        "    \\\\ &=\n",
        "    \\begin{bmatrix}\n",
        "    1 * 0.11 + 1 * 0.21 & 1 * 0.12 + 1 * 0.08\n",
        "    \\end{bmatrix}\n",
        "    \\cdot\n",
        "    \\begin{bmatrix}\n",
        "    0.14 \\\\ \n",
        "    0.15\n",
        "    \\end{bmatrix}\n",
        "    \\\\ &=\n",
        "    \\begin{bmatrix}\n",
        "    0.32 & 0.20\n",
        "    \\end{bmatrix}\n",
        "    \\cdot\n",
        "    \\begin{bmatrix}\n",
        "    0.14 \\\\ \n",
        "    0.15 \n",
        "    \\end{bmatrix}\n",
        "    \\\\ &=\n",
        "    \\begin{bmatrix}\n",
        "    0.32 * 0.14 + 0.20 * 0.15\n",
        "    \\end{bmatrix}\n",
        "    \\\\ &= 0.0748 \n",
        "    \\end{aligned}\n",
        "    \\\\\n",
        "    $\n",
        "\n",
        "3. Compute the error (and more importantly, the delta).\n",
        "\n",
        "    $\\begin{aligned}\n",
        "    L_2Error &= (0 - 0.0748)^2 \\\\\n",
        "    &= 0.0056 \\\\\n",
        "    \\Delta_{o_1} &= (0 - 0.0748) \\\\\n",
        "    &= -0.0748 \\\\\n",
        "    \\end{aligned}$\n",
        "\n",
        "4. Backpropagate updates back through the network, assuming: \n",
        "    $learning\\_rate = 0.05$; \n",
        "    identity activation functions for all nodes.\n",
        "     \n",
        "    $\\begin{aligned}\n",
        "    \\begin{bmatrix}\n",
        "    w_{h_1, o_1} \\\\ \n",
        "    w_{h_2, o_1}\n",
        "    \\end{bmatrix} &\\leftarrow \n",
        "    \\begin{bmatrix}\n",
        "    0.14 \\\\ \n",
        "    0.15 \n",
        "    \\end{bmatrix} + 0.05 \\cdot \n",
        "    \\begin{bmatrix}\n",
        "    0.32 \\\\ \n",
        "    0.20 \n",
        "    \\end{bmatrix} \\cdot 1.0 \\cdot -0.0748 \\\\\\\\\n",
        "    &= \n",
        "    \\begin{bmatrix}\n",
        "    0.14 \\\\ \n",
        "    0.15 \n",
        "    \\end{bmatrix} + \n",
        "    \\begin{bmatrix}\n",
        "    0.05 * 0.32 * 1.0 * -0.0748 \\\\\n",
        "    0.05 * 0.20 * 1.0 * -0.0748 \n",
        "    \\end{bmatrix} \\\\\n",
        "    &= \n",
        "    \\begin{bmatrix}\n",
        "    0.14 \\\\ \n",
        "    0.15 \n",
        "    \\end{bmatrix} +\n",
        "    \\begin{bmatrix}\n",
        "    -0.0011968 \\\\\n",
        "    -0.000748 \n",
        "    \\end{bmatrix} \\\\\n",
        "    &=\n",
        "    \\begin{bmatrix}\n",
        "    0.1388 \\\\ \n",
        "    0.1493\n",
        "    \\end{bmatrix}\n",
        "    \\end{aligned}$\n",
        "\n",
        "    $\\begin{aligned}\n",
        "    \\begin{bmatrix}\n",
        "    w_{i_1,h_1} & w_{i_1,h_2} \\\\ \n",
        "    w_{i_2,h_1} & w_{i_2,h_2}\n",
        "    \\end{bmatrix} &\\leftarrow \n",
        "    \\begin{bmatrix}\n",
        "    0.11 & 0.12 \\\\\n",
        "    0.21 & 0.08\n",
        "    \\end{bmatrix} + 0.05 \\cdot\n",
        "    \\begin{bmatrix}\n",
        "    1 & 1 \\\\ \n",
        "    1 & 1\n",
        "    \\end{bmatrix} \\cdot 1.0 \\odot\n",
        "    \\begin{bmatrix}\n",
        "    0.14 & 0.15 \\\\ \n",
        "    0.14 & 0.15\n",
        "    \\end{bmatrix} \\cdot -0.0748 \\\\ &=\n",
        "    \\begin{bmatrix}\n",
        "    0.11 & 0.12 \\\\\n",
        "    0.21 & 0.08\n",
        "    \\end{bmatrix} + \n",
        "    \\begin{bmatrix}\n",
        "    0.05 * 1 * 1.0 & 0.05 * 1 * 1.0 \\\\ \n",
        "    0.05 * 1 * 1.0 & 0.05 * 1 * 1.0 \\\\ \n",
        "    \\end{bmatrix} \\odot \n",
        "    \\begin{bmatrix}\n",
        "    0.14 * -0.0748 & 0.15 * -0.0748\\\\ \n",
        "    0.14 * -0.0748 & 0.15 * -0.0748 \n",
        "    \\end{bmatrix} \\\\ &=\n",
        "    \\begin{bmatrix}\n",
        "    0.11 & 0.12 \\\\\n",
        "    0.21 & 0.08\n",
        "    \\end{bmatrix} + \n",
        "    \\begin{bmatrix}\n",
        "    0.05 & 0.05 \\\\ \n",
        "    0.05 & 0.05 \n",
        "    \\end{bmatrix} \\odot \n",
        "    \\begin{bmatrix}\n",
        "    -0.010472 & -0.01122 \\\\\n",
        "    -0.010472 & -0.01122\n",
        "    \\end{bmatrix} \\\\ &=\n",
        "    \\begin{bmatrix}\n",
        "    0.11 & 0.12 \\\\\n",
        "    0.21 & 0.08\n",
        "    \\end{bmatrix} + \n",
        "    \\begin{bmatrix}\n",
        "    0.05 * -0.010472 & 0.05 *  -0.01122 \\\\\n",
        "    0.05 * -0.010472 & 0.05 * -0.01122\n",
        "    \\end{bmatrix} \\\\ &=\n",
        "    \\begin{bmatrix}\n",
        "    0.11 & 0.12 \\\\\n",
        "    0.21 & 0.08\n",
        "    \\end{bmatrix} +     \n",
        "    \\begin{bmatrix}\n",
        "    -0.0005236 & -0.000561 \\\\\n",
        "    -0.0005236 & -0.000561\n",
        "    \\end{bmatrix} \\\\ &= \n",
        "    \\begin{bmatrix}\n",
        "    0.1095 & 0.1194 \\\\\n",
        "    0.2095 & 0.0794\n",
        "    \\end{bmatrix}  \n",
        "    \\end{aligned}$\n"
      ]
    },
    {
      "metadata": {
        "id": "A9_A4TmfH8sf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Part 3"
      ]
    },
    {
      "metadata": {
        "id": "3IJae_00CNGd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "quSNvv7QGJVO",
        "colab_type": "code",
        "outputId": "73e30808-a5c6-4fae-e8c9-18871f3bbadf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "cell_type": "code",
      "source": [
        "print(\n",
        "    f'training images \\\n",
        "        \\n\\tcount: {len(train_labels)} \\\n",
        "        \\n\\tshape: {train_images.shape} \\\n",
        "        \\n\\timage data type: {train_images.dtype} \\\n",
        "        \\n\\tlabel data type: {train_labels.dtype}\\n',\n",
        "    f'testing images \\\n",
        "        \\n\\tcount: {len(test_labels)} \\\n",
        "        \\n\\tshape: {test_images.shape}\\n',\n",
        "    f'example: \\\n",
        "        \\n\\tpixel value: {train_images[0,5,20]} \\\n",
        "        \\n\\tlabel value: {train_labels[0]}\\n',\n",
        "    'example image:'\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "digit_image = train_images[0]\n",
        "plt.imshow(digit_image, cmap=plt.cm.binary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training images         \n",
            "\tcount: 60000         \n",
            "\tshape: (60000, 28, 28)         \n",
            "\timage data type: uint8         \n",
            "\tlabel data type: uint8\n",
            " testing images         \n",
            "\tcount: 10000         \n",
            "\tshape: (10000, 28, 28)\n",
            " example:         \n",
            "\tpixel value: 23         \n",
            "\tlabel value: 9\n",
            " example image:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe3513d54e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nApToIglQoosk4P8BEFaOWmLPvzsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "3m90OXEiGYq4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yo_ySpzNGkIm",
        "colab_type": "code",
        "outputId": "3e2e979a-c277-4c77-a0c1-adff7e7e8e70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# Configure a convnet with 3 layers of convolutions and max pooling.\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "\n",
        "# Add layers to flatten the 2D image and then do a 10-way classification.\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Sat5heiVGmIX",
        "colab_type": "code",
        "outputId": "8cce59e1-90a1-4350-d85c-7ed64fcea804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=64)\n",
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 65s 1ms/step - loss: 0.5785 - acc: 0.7866\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 65s 1ms/step - loss: 0.3609 - acc: 0.8684\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 64s 1ms/step - loss: 0.3092 - acc: 0.8857\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 64s 1ms/step - loss: 0.2781 - acc: 0.8982\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 64s 1ms/step - loss: 0.2600 - acc: 0.9049\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 64s 1ms/step - loss: 0.2461 - acc: 0.9098\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 65s 1ms/step - loss: 0.2318 - acc: 0.9149\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 65s 1ms/step - loss: 0.2240 - acc: 0.9177\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 65s 1ms/step - loss: 0.2157 - acc: 0.9200\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 65s 1ms/step - loss: 0.2116 - acc: 0.9216\n",
            "10000/10000 [==============================] - 3s 347us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2634700183570385, 0.9078]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}